{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ef652d45-5011-4322-bc27-4cf4976d7ace",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Psychic LLM?\"\n",
    "description: \"Can ChatGPT Infer What I Want Without Being Told?\" \n",
    "author: \"Emma\"\n",
    "date: \"12/17/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d154d0c-c90a-4f60-a5cc-21185f55d600",
   "metadata": {},
   "source": [
    "![](pyshic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e23f60c-1e54-4294-ad7f-d53778cfa638",
   "metadata": {},
   "source": [
    "## Description\n",
    "This post explores whether an LLM can infer user intent when instructions are vague or incomplete. The task mirrors real-world communication, where meaning is often implied rather than explicit\n",
    "\n",
    "The objective of this post is to test how an LLM handles underspecified requests and whether it asks clarifying questions or makes assumptions.\n",
    "\n",
    "**Step 1: Giving an Underspecified Prompt**\n",
    "I began by providing a prompt that lacks key details. This tests whether the model seeks clarification or defaults to interpretation.\n",
    "\n",
    "**Prompt 1:**\n",
    "I need help with something important.\n",
    "\n",
    "**Response:**\n",
    "Of course. I’m here—tell me what’s going on and how I can help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3852f00-68e3-42fa-95bf-411812cbbb69",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "I tried multiple times pushing back with phrases like \"you know what I am talking about\" or \"just help me\" but still the LLM pushed back and politely requested more context. This builds upon my previous post and creates emphasis on context: if context is given confidently and fully enough for GPT to provide an explanation (even if the context is incorrect or nonexistent) it is able to function, but if provided with an incomplete thought or incomplete context the LLM is unsure how to proceed and will keep requesting adequate information in order to answer the question. These are two interesting phenomena that I have observed while using LLM's in my own time: both GPT creating false ideas and GPT's \"stubborness\" when requesting and needing more context to continue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0be27ede-fe04-4aed-a167-58073e0a4668",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"LLMS and Vague Context\"\n",
    "description: \"When Ambiguous Prompts Lead to Confident Answers\" \n",
    "author: \"Emma\"\n",
    "date: \"11/20/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed67a980-e733-4dd9-94a1-a5b32d528121",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "This post examines how LLMs respond to vague or underspecified prompts. Rather than requesting clarification, the model often generates confident explanations.\n",
    "\n",
    "**Goal**\n",
    "This post tests the modelâ€™s tolerance for ambiguity and its tendency to prioritize fluency over uncertainty.\n",
    "\n",
    "## Prompt\n",
    "Since LLM's are known to function almost entirely off of the context they are given (opposed to scouring the web for information and information they obtain), so I wanted to see how GPT acts with limited or confusing context. I inputted the prompt: \"Explain why this is happening\" and kept prompting it to answer when it asked for more information.\n",
    "\n",
    "### Reflection\n",
    "Over repeated efforts, GPT clearly is unable to move forward without necessary context. I have seen this happen before in an actual instance where I did not provide enough information, and in a test case scenario proves that right.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
